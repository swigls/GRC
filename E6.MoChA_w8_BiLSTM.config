#!crnn/rnn.py
# kate: syntax python;
# -*- mode: python -*-
# sublime: syntax 'Packages/Python Improved/PythonImproved.tmLanguage'
# vim:set expandtab tabstop=4 fenc=utf-8 ff=unix ft=python:

import os
import numpy
from subprocess import check_output, CalledProcessError
from Pretrain import WrapEpochValue
import math
import tensorflow as tf

#
gsa_trainable = True 

## important
eval_use_train_flag = False

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True
setup_name = os.path.splitext(os.path.basename(__file__))[0]

debug_mode = False
if int(os.environ.get("RETURNN_DEBUG", "0")):
    print("** DEBUG MODE")
    debug_mode = True

if config.has("beam_size"):
    beam_size = config.int("beam_size", 0)
    print("** beam_size %i" % beam_size)
else:
    beam_size = 12

# data
num_inputs = 40
num_outputs = {"classes": (10025, 1), "data": (num_inputs, 2)}  # see vocab
EpochSplit = 20


def get_dataset(key, subset=None, train_partition_epoch=None):
    d = {
        'class': 'LibriSpeechCorpus',
        'path': 'data/dataset-ogg', #/ogg-zips',
        "use_zip": True,
        "use_ogg": True,
        "use_cache_manager": not debug_mode,
        "prefix": key,
        "bpe": {
            'bpe_file': 'data/dataset/trans.bpe.codes',
            'vocab_file': 'data/dataset/trans.bpe.vocab',
            'seq_postfix': [0],
            'unknown_label': '<unk>'},
        "audio": {
            "norm_mean": "data/dataset/stats.mean.txt",
            "norm_std_dev": "data/dataset/stats.std_dev.txt"},
    }
    if key.startswith("train"):
        d["partition_epoch"] = train_partition_epoch
        if key == "train":
            d["epoch_wise_filter"] = {
                (1, 5): {
                    'use_new_filter': True,
                    'max_mean_len': 50,  # chars
                    'subdirs': ['train-clean-100', 'train-clean-360']},
                (5, 10): {
                    'use_new_filter': True,
                    'max_mean_len': 150,  # chars
                    'subdirs': ['train-clean-100', 'train-clean-360']},
                (11, 20): {
                    'use_new_filter': True,
                    'subdirs': ['train-clean-100', 'train-clean-360']},
                }
        #d["audio"]["random_permute"] = True
        num_seqs = 281241  # total
        d["seq_ordering"] = "laplace:%i" % (num_seqs // 1000)
    else:
        d["fixed_random_seed"] = 1
        d["seq_ordering"] = "sorted_reverse"
    if subset:
        d["fixed_random_subset"] = subset  # faster
    return d

train = get_dataset("train", train_partition_epoch=EpochSplit)
dev = get_dataset("dev", subset=1500)
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
target = "classes"
EncKeyTotalDim = 1024
AttNumHeads = 1
EncKeyPerHeadDim = EncKeyTotalDim // AttNumHeads
EncValueTotalDim = 2048
EncValuePerHeadDim = EncValueTotalDim // AttNumHeads
LstmDim = EncValueTotalDim // 2

def add_lc_bw_lstm_layer(n_units, Nc=64, Nr=32, prefix="", _from=""):
    net = network
    ### Assuems input is (B,T,F) or (T,B,F)
    net[prefix+"_bw_in_strwin"] = { "class": "strided_window", "axis":"T", "window_size":Nc+Nr, "stride":Nc, "from": [_from]} #(B,[T/Nc],Nc+Nr,F)
    net[prefix+"_bw_in_merge"] = { "class": "merge_dims", "axes":["B","T"], "from": [prefix+"_bw_in_strwin"]} #(B*[T/Nc],Nc+Nr,F)
    net[prefix+"_bw"] = { "class": "rec", "unit": "nativelstm2", "n_out" : n_units, "direction": -1, "from": [prefix+"_bw_in_merge"], "trainable":gsa_trainable} #(Nc+Nr,B*[T/Nc],F)
    net[prefix+"_bw_out_slice"] = { "class": "slice", "axis":"T", "slice_end":Nc, "from": [prefix+"_bw"]} #(Nc,B*[T/Nc],F)
    net[prefix+"_bw_out_split"] = { "class": "split_batch_time", "base": prefix+"_bw_in_strwin", "from": [prefix+"_bw_out_slice"]} #(B,[T/Nc],Nc,F)
    net[prefix+"_bw_out_merge"] = { "class": "merge_dims", "axes":[1,2], "from": [prefix+"_bw_out_split"]} #(B,T',F)
    net[prefix+"_bw_out"] = { "class": "reinterpret_and_crop", "size_base":_from, "from": [prefix+"_bw_out_merge"]} #(B,T,F)


network = {
"source": {"class": "eval", "eval": "tf.clip_by_value(source(0), -3.0, 3.0)"},

"lstm0_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["source"], "trainable":gsa_trainable},
"lstm0_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["source"], "trainable":gsa_trainable},
"lstm0_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (3,), "from": ["lstm0_fw", "lstm0_bw"], "trainable": False},
#"lstm0_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (3,), "from": ["lstm0_fw", "lstm0_bw_out"], "trainable": False},

"lstm1_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["lstm0_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm1_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["lstm0_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm1_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (2,), "from": ["lstm1_fw", "lstm1_bw"], "trainable": False},
#"lstm1_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (2,), "from": ["lstm1_fw", "lstm1_bw_out"], "trainable": False},

"lstm2_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["lstm1_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm2_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["lstm1_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm2_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["lstm2_fw", "lstm2_bw"], "trainable": False},
#"lstm2_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["lstm2_fw", "lstm2_bw_out"], "trainable": False},

"lstm3_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["lstm2_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm3_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["lstm2_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm3_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["lstm3_fw", "lstm3_bw"], "trainable": False},
#"lstm3_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["lstm3_fw", "lstm3_bw_out"], "trainable": False},

"lstm4_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["lstm3_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm4_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["lstm3_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm4_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["lstm4_fw", "lstm4_bw"], "trainable": False},
#"lstm4_pool": {"class": "pool", "mode": "max", "padding": "same", "pool_size": (1,), "from": ["lstm4_fw", "lstm4_bw_out"], "trainable": False},

"lstm5_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": 1, "from": ["lstm4_pool"], "dropout": 0.3, "trainable":gsa_trainable },
"lstm5_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : LstmDim, "direction": -1, "from": ["lstm4_pool"], "dropout": 0.3, "trainable":gsa_trainable },

"encoder": {"class": "copy", "from": ["lstm5_fw","lstm5_bw"]},  # dim: EncValueTotalDim
#"encoder": {"class": "copy", "from": ["lstm5_fw","lstm5_bw_out"]},  # dim: EncValueTotalDim
"enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": EncKeyTotalDim, "trainable":gsa_trainable},  # preprocessed_attended in Blocks
"chunk_enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": EncKeyTotalDim},  # preprocessed_attended in Blocks
"inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": AttNumHeads, "trainable":gsa_trainable}, #(enc-T, B ,H)?
"enc_value": {"class": "split_dims", "axis": "F", "dims": (AttNumHeads, EncValuePerHeadDim), "from": ["encoder"]},  # (B, enc-T, H, D'/H)

############################### ADDED #############################################
"output": {"class": "rec", "from": [], 'cheating': config.bool("cheating", False), "unit": {
    'output': {'class': 'choice', 'target': target, 'beam_size': beam_size, 'cheating': config.bool("cheating", False), 'from': ["output_prob"], "initial_output": 0}, 
    "end": {"class": "compare", "from": ["output"], "value": 0},
    'target_embed': {'class': 'linear', 'activation': None, "with_bias": False, 'from': ['output'], "n_out": 621, "initial_output": 0, "trainable":gsa_trainable},  # feedback_input 
    "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": EncKeyTotalDim, "trainable":gsa_trainable}, 
    "chunk_weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": EncKeyTotalDim}, 
    "s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["s"], "n_out": EncKeyTotalDim, "trainable":gsa_trainable}, 
    "chunk_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["s"], "n_out": EncKeyTotalDim}, 
    "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "s_transformed"], "n_out": EncKeyTotalDim, "trainable":gsa_trainable}, 
    "chunk_energy_in": {"class": "combine", "kind": "add", "from": ["base:chunk_enc_ctx", "chunk_weight_feedback", "chunk_s_transformed"], "n_out": EncKeyTotalDim}, 
    "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
    "chunk_energy_tanh": {"class": "activation", "activation": "tanh", "from": ["chunk_energy_in"]},
    #"energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": AttNumHeads, "trainable":gsa_trainable},  # (B, enc-T, H)
    "energy": {"class": "linear", "activation": None, "with_bias": True, "bias_init": 0.0, \
        "from": ["energy_tanh"], "n_out": AttNumHeads},  # (B, enc-T, H) 
    "chunk_energy": {"class": "linear", "activation": None, "with_bias": False, \
        "from": ["chunk_energy_tanh"], "n_out": AttNumHeads},  # (B, enc-T, H) 


    # Windowed attention
    # (T, B)
    #"p_t": {"class": "eval", "from": "p_t_in", "eval": "tf.to_float(source(0))"},
    #"p_t_in": {"class": "reduce", "from": "prev:att_weights", "mode":"argmax", "axis":"t"},
    #"energy_reinterpreted": {"class": "reinterpret_data", "enforce_batch_major": True, "from": "energy", "trainable": False},
    #"att_weights": {"class": "softmax_over_spatial", "from": ["energy_reinterpreted"], "window_start": "p_t_in", "window_size": 20, "is_output_layer":True},  

    # MoChA
    "att_weights": {"class": "monotonic_hard_attention_2", "from": ["energy","prev:att_weights", "chunk_energy"],\
        "sigmoid_noise": 1.0, "train_cumcalc_mode": "parallel",\
        "chunk_size": 8, "test_same_as_train":False,
        "initial_output": "zeros"},  # (B, enc-T, H)

    # GRC
    #"att_weights": {"class": "gated_recurrent_context", "from": ["energy"],\
    #    "initial_output": "zeros", "infer_threshold":0.01, "first_reset_value":1., "exp_energy_cumsum": True, #False,
    #    "sigmoid_energy_cumprod":False,},  # (B, enc-T, H) 
   
    # GSA
    #"att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},  # (B, enc-T, H)

    "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:inv_fertility"],
        "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": AttNumHeads, "shape": (None, AttNumHeads)}}, #BY HSLEE
    "att0": {"class": "generic_attention", "weights": "att_weights", "base": "base:enc_value"},  # (B, H, V)
    "att": {"class": "merge_dims", "axes": "except_batch", "from": ["att0"]},  # (B, H*V)
    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:target_embed", "prev:att"], "n_out": 1000, "trainable":gsa_trainable},  # transform 
    "readout_in": {"class": "linear", "from": ["s", "prev:target_embed", "att"], "activation": None, "n_out": 1000, "trainable":gsa_trainable},  # merge + post_merge bias 
    "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"]}, 
    "output_prob": {
        "class": "softmax", "from": ["readout"], "dropout": 0.3,
        "target": target, "loss": "ce", "loss_opts": {"label_smoothing": 0.1, "scale": 1.}, "trainable":gsa_trainable} 
}, "target": target, "max_seq_len": "max_len_from('base:encoder')"},
###################################################################################
"decision": {
    "class": "decide", "from": ["output"], "loss": "edit_distance", "target": target,
    "loss_opts": {
        #"debug_print": True
        }
    },

"ctc": {"class": "softmax", "from": ["encoder"], "loss": "ctc", "target": target,
    "loss_opts": {"beam_width": 1, "scale":1., "ctc_opts": {"ignore_longer_outputs_than_inputs": True}}}
}

###### ADDED #######

# LCBLSTM layers
#add_lc_bw_lstm_layer(LstmDim, Nc=72, Nr=36, prefix="lstm0", _from="source")
#add_lc_bw_lstm_layer(LstmDim, Nc=24, Nr=12, prefix="lstm1", _from="lstm0_pool")
#add_lc_bw_lstm_layer(LstmDim, Nc=12, Nr=6, prefix="lstm2", _from="lstm1_pool")
#add_lc_bw_lstm_layer(LstmDim, Nc=12, Nr=6, prefix="lstm3", _from="lstm2_pool")
#add_lc_bw_lstm_layer(LstmDim, Nc=12, Nr=6, prefix="lstm4", _from="lstm3_pool")
#add_lc_bw_lstm_layer(LstmDim, Nc=12, Nr=6, prefix="lstm5", _from="lstm4_pool")
####################

#preload_from_files = {
#  "name1": {
#     "filename": "data/exp-*/model.***",
#     "prefix": "",
#     "ignore_missing": True,
#     "init_for_train": True,
#   },
#}

search_output_layer = "decision"
debug_print_layer_output_template = True

# trainer
batching = "random"
log_batch_size = True
batch_size =  18000 
max_seqs = 200
max_seq_length = {"classes": 75}
#chunking = ""  # no chunking
truncation = -1

def custom_construction_algo(idx, net_dict):
    # For debugging, use: python3 ./crnn/Pretrain.py config... Maybe set repetitions=1 below.
    StartNumLayers = 2
    InitialDimFactor = 0.5
    orig_num_lstm_layers = 0
    while "lstm%i_fw" % orig_num_lstm_layers in net_dict:
        orig_num_lstm_layers += 1
    assert orig_num_lstm_layers >= 2
    idx = max(idx - 1, 0)  # repeat first
    num_lstm_layers = idx + StartNumLayers  # idx starts at 0. start with N layers
    if idx == 0:  # initially disable dropout
        net_dict["lstm%i_fw" % (orig_num_lstm_layers - 1)]["dropout"] = 0
        net_dict["lstm%i_bw" % (orig_num_lstm_layers - 1)]["dropout"] = 0
        #net_dict["lstm%i_bw_main" % (orig_num_lstm_layers - 1)]["dropout"] = 0
    else:
        num_lstm_layers -= 1  # repeat like idx=0, but now with dropout
    if num_lstm_layers > orig_num_lstm_layers:
        # Finish. This will also use label-smoothing then.
        return None
    # Skip to num layers.
    net_dict["encoder"]["from"] = ["lstm%i_fw" % (num_lstm_layers - 1), "lstm%i_bw" % (num_lstm_layers - 1)]
    #net_dict["encoder"]["from"] = ["lstm%i_fw" % (num_lstm_layers - 1), "lstm%i_bw_out" % (num_lstm_layers - 1)]
    # Delete non-used lstm layers. This is not explicitly necessary but maybe nicer.
    for i in range(num_lstm_layers, orig_num_lstm_layers):
        del net_dict["lstm%i_fw" % i]
        del net_dict["lstm%i_bw" % i]
        #for key in search_keys_in_dict(net_dict, "lstm%i_bw_out" % i):
        #  del net_dict[key]
        del net_dict["lstm%i_pool" % (i - 1)]
    # Thus we have layers 0 .. (num_lstm_layers - 1).
    layer_idxs = list(range(0, num_lstm_layers))
    layers = ["lstm%i_fw" % i for i in layer_idxs] + ["lstm%i_bw" % i for i in layer_idxs]
    grow_frac = 1.0 - float(orig_num_lstm_layers - num_lstm_layers) / (orig_num_lstm_layers - StartNumLayers)
    dim_frac = InitialDimFactor + (1.0 - InitialDimFactor) * grow_frac
    for layer in layers:
        net_dict[layer]["n_out"] = int(net_dict[layer]["n_out"] * dim_frac)
        if "dropout" in net_dict[layer]:
            net_dict[layer]["dropout"] *= dim_frac
    net_dict["enc_value"]["dims"] = (AttNumHeads, int(EncValuePerHeadDim * dim_frac * 0.5) * 2)
    # Use label smoothing only at the very end.
    net_dict["output"]["unit"]["output_prob"]["loss_opts"]["label_smoothing"] = 0.
    return net_dict

def search_keys_in_dict(d, prefix="", postfix=""):
    return [key for key,val in d.items() if key.startswith(prefix) and key.endswith(postfix)]

#pretrain = {"repetitions": 5, "copy_param_mode": "subset", "construction_algo": custom_construction_algo}
pretrain = {"repetitions": 25, "copy_param_mode": "subset", "construction_algo": custom_construction_algo}

num_epochs = 350 #270 
model = "data/exp-%s/model" % setup_name #"net-model/network"
cleanup_old_models = True
gradient_clip = 0
#gradient_clip_global_norm = 1.0
adam = True
optimizer_epsilon = 1e-8
#debug_add_check_numerics_ops = True
#debug_add_check_numerics_on_output = True
stop_on_nonfinite_train_score = False
tf_log_memory_usage = True
debug_grad_summaries = True
gradient_noise = 0.0
learning_rate = 0.0008 #1e-5 
learning_rates = list(numpy.linspace(0.0003, learning_rate, num=10))  # warmup
#learning_rates = [1e-5]  # warmup
learning_rate_control = "newbob_multi_epoch"
#learning_rate_control_error_measure = "dev_score_output"
learning_rate_control_relative_error_relative_lr = True
learning_rate_control_min_num_epochs_per_new_lr = 3
use_learning_rate_control_always = True
newbob_multi_num_epochs = EpochSplit
newbob_multi_update_interval = 1
newbob_learning_rate_decay = 0.9
learning_rate_file = "data/exp-%s/train-scores.data" % setup_name #"newbob.data"

# log
log = "data/exp-%s/returnn.%s.$date.log" % (setup_name, task) #"log/crnn.%s.log" % task
log_verbosity = 5

